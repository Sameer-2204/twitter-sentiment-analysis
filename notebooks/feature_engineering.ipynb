{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "036b69b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded and cleaned\n",
      "                                                text  \\\n",
      "0  Here are Thursday's biggest analyst calls: App...   \n",
      "1  Buy Las Vegas Sands as travel to Singapore bui...   \n",
      "2  Piper Sandler downgrades DocuSign to sell, cit...   \n",
      "3  Analysts react to Tesla's latest earnings, bre...   \n",
      "4  Netflix and its peers are set for a â€˜return to...   \n",
      "\n",
      "                                          clean_text  \n",
      "0  thursday biggest analyst call apple amazon tes...  \n",
      "1  buy la vega sand travel singapore build well f...  \n",
      "2  piper sandler downgrade docusign sell citing e...  \n",
      "3  analyst react tesla latest earnings break what...  \n",
      "4  netflix peer set return growth analyst say giv...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "\n",
    "train_df=pd.read_csv(\"../data/train_data.csv\")\n",
    "valid_df=pd.read_csv(\"../data/valid_data.csv\")\n",
    "\n",
    "\n",
    "\n",
    "from scripts.data_cleaning import clean_text\n",
    "\n",
    "if \"clean_text\" not in train_df.columns:\n",
    "    train_df[\"clean_text\"]=train_df[\"text\"].astype(str).apply(clean_text)\n",
    "    valid_df[\"clean_text\"]=valid_df[\"text\"].astype(str).apply(clean_text)\n",
    "\n",
    "print(\"Data loaded and cleaned\")\n",
    "print(train_df[[\"text\", \"clean_text\"]].head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bd20230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: h:\\twitter_analysis\n",
      "scripts module found at: h:\\twitter_analysis\\scripts\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "import scripts\n",
    "print(\"scripts module found at:\", scripts.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "569fc7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Feature Shape:  (16990, 5000) (4117, 5000)\n",
      "TF-IDF Veactorizer saved.\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF Vectorization\n",
    "\n",
    "tfidf=TfidfVectorizer(max_features=5000)\n",
    "\n",
    "X_train_tfidf=tfidf.fit_transform(train_df[\"clean_text\"])\n",
    "X_valid_tfidf=tfidf.transform(valid_df[\"clean_text\"])\n",
    "\n",
    "y_train=train_df[\"label\"]\n",
    "y_valid=valid_df[\"label\"]\n",
    "\n",
    "print(\"TF-IDF Feature Shape: \", X_train_tfidf.shape,X_valid_tfidf.shape)\n",
    "\n",
    "with open(\"../models/tfidf_vectorizer.pkl\",\"wb\") as f:\n",
    "    pickle.dump(tfidf,f)\n",
    "print(\"TF-IDF Veactorizer saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7423d1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded Sequences Shape:  (16990, 50) (4117, 50)\n",
      "tokenizer saved\n"
     ]
    }
   ],
   "source": [
    "#Keras Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "VOCAB_SIZE=5000\n",
    "MAX_LENGTH=50\n",
    "\n",
    "tokenizer=Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_df[\"clean_text\"])\n",
    "\n",
    "X_train_seq=tokenizer.texts_to_sequences(train_df[\"clean_text\"])\n",
    "X_valid_seq=tokenizer.texts_to_sequences(valid_df[\"clean_text\"])\n",
    "\n",
    "X_train_padded=pad_sequences(X_train_seq, maxlen=MAX_LENGTH, padding=\"post\", truncating=\"post\")\n",
    "X_valid_padded=pad_sequences(X_valid_seq, maxlen=MAX_LENGTH, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "print(\"Padded Sequences Shape: \", X_train_padded.shape, X_valid_padded.shape)\n",
    "\n",
    "with open(\"../models/tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print(\"tokenizer saved\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
