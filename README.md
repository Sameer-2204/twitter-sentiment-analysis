# ğŸ“Š Twitter Sentiment Analysis  
### End-to-End NLP Pipeline using Machine Learning & Transformer Models

---

## ğŸš€ Project Overview

This project implements a complete end-to-end Natural Language Processing (NLP) pipeline for sentiment classification on Twitter data.

The system compares:

- Classical Machine Learning models  
- Deep Learning architectures  
- Transformer-based models (DistilBERT)

The objective is to evaluate performance differences and build a clean, modular, and reproducible NLP workflow similar to real-world ML projects.

---

## ğŸ¯ Objectives

- Perform sentiment classification on Twitter text data  
- Compare ML, Deep Learning, and Transformer models  
- Fine-tune DistilBERT for contextual understanding  
- Evaluate models using proper classification metrics  
- Maintain structured and reusable code  

---

## ğŸ§  Models Implemented

### ğŸ”¹ Classical ML
- Logistic Regression (TF-IDF features)
- Random Forest
- XGBoost
- Naive Bayes
- Linear SVM 

### ğŸ”¹ Deep Learning

- CNN  
- LSTM  
- BiLSTM  

### ğŸ”¹ Transformer Model
- DistilBERT (Fine-tuned)

---

## ğŸ”„ NLP Pipeline

1. Data Cleaning  
   - URL removal  
   - Special character removal  
   - Lowercasing  
   - Stopword removal  

2. Tokenization  
   - Custom tokenizer for ML models  
   - HuggingFace tokenizer for DistilBERT  

3. Feature Engineering  
   - TF-IDF vectorization  
   - Sequence padding  

4. Model Training  
   - Train-validation split  
   - Supervised learning  

5. Evaluation  
   - Accuracy  
   - Precision  
   - Recall  
   - F1-score  
   - Confusion Matrix  

---

## ğŸ“ˆ Model Performance

| Model                | Accuracy |
|----------------------|----------|
| Logistic Regression  | 79%      |
| CNN                  | 64%      |
| LSTM                 | 64%      |
| BiLSTM               | 71%      |
| DistilBERT           | **XX%**  |

> Replace XX% with your actual results before publishing.

---

## ğŸ— Project Structure
twitter_analysis/
â”‚
â”œâ”€â”€ data/ # Raw and processed datasets
â”œâ”€â”€ models/ # Saved trained models (ignored in Git)
â”œâ”€â”€ notebooks/ # EDA and experimentation notebooks
â”œâ”€â”€ scripts/ # Training and preprocessing scripts
â”‚ â”œâ”€â”€ data_cleaning.py
â”‚ â”œâ”€â”€ visualization.py
â”‚ â”œâ”€â”€ train_logistic.py
â”‚ â”œâ”€â”€ train_lstm.py
â”‚ â”œâ”€â”€ train_cnn.py
â”‚ â””â”€â”€ train_distilbert.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md


> Note: Large trained model files are excluded due to GitHub size limitations. Models can be regenerated using the training scripts.

---

## ğŸ›  Tech Stack

- Python  
- Scikit-learn  
- TensorFlow / Keras  
- PyTorch  
- HuggingFace Transformers  
- Pandas  
- NumPy  
- Matplotlib / Seaborn  

---
## â–¶ï¸ How to Run
python notebooks/model_training.ipynb


---

## ğŸ“Š Exploratory Data Analysis

EDA includes:
- Sentiment distribution visualization  
- Text length analysis  
- Word frequency analysis  

Notebooks are available in the `/notebooks` directory.

---

## ğŸ”® Future Improvements

- Hyperparameter tuning  
- Model deployment using FastAPI  
- Docker containerization  
- Real-time inference API  

---

## ğŸ‘¤ Author

**Sameer Tripathi**  
Aspiring AI/ML Engineer | Data Science Enthusiast
